.. vim: set fileencoding=utf-8 :
.. date: Wed 06 Dec 2017 11:07:29 CET

.. _bob.bio.vein.preprocessing:

=========================
 Preprocessing Utilities
=========================

This package includes utilities to visualize and model the preprocessing stage
of vein recognition pipelines.


Region of Interest Benchmarking
-------------------------------

Automatic region of interest (RoI) finding and cropping can be evaluated using
a couple of scripts available in this package. The first thing to do is to
generate a reference, which typically is done using annotations. A set of
special configurations, which avoid the alignment normalization is available as
part of this package, making this process relatively easy.

To generate a reference set of masks from annotations, run the following
command:

.. code-block:: sh

   $ verify.py -vvv -s "benchmark-annotations" verafinger benchmark_annotations -o preprocessing


Repeat the above command for each combination of database/protocol you may need
to compute a set of reference images.

Once you have generated a reference, you can test the methods of your interest.
For example, to benchmark the default settings for a Watershed-based method,
run the following preprocessor:

.. code-block:: sh

   $ verify.py -vvv -s "benchmark-watershed" verafinger benchmark_watershed -o preprocessing


.. tip::

   If you have more processing cores on your local machine and don't want to
   submit your job for SGE execution, you can run it in parallel (using 4
   parallel tasks) by adding the options ``--parallel=4 --nice=10``.

   Optionally, you may use the ``parallel`` resource configuration which
   already sets the number of parallel jobs to the number of hardware cores you
   have installed on your machine (as with
   :py:func:`multiprocessing.cpu_count`) and sets ``nice=10``. For example:

   .. code-block:: sh

      $ verify.py -vvv -s "benchmark-watershed" verafinger benchmark_watershed parallel -o preprocessing

   To run on the Idiap SGE grid using our stock
   io-big-48-slots-4G-memory-enabled (see
   :py:mod:`bob.bio.vein.configurations.gridio4g48`) configuration, use:

   .. code-block:: sh

      $ verify.py -vvv -s "benchmark-watershed" verafinger benchmark_watershed parallel grid -o preprocessing

   You may also, optionally, use the configuration resource ``gridio4g48``,
   which is just an alias of ``grid`` in this package.


The program ``compare_rois.py`` compares two sets of ``preprocessed`` images
and masks, generated by *different* preprocessors (see
:py:class:`bob.bio.base.preprocessor.Preprocessor`) and calculates a few
metrics to help you determine how both techniques compare. Normally, the
program is used to compare the result of automatic RoI detection to manually
annoted regions on the same images. To use it, just point it to the outputs of
two experiments representing the manually annotated regions and automatically
extracted ones. E.g.:

.. code-block:: sh

   $ compare_rois.py <path-to>/verafinger/benchmark-annotations/preprocessed <path-to>/verafinger/benchmark-watershed/preprocessed
   Statistics:
   Jaccard index: 9.84e-01 +- 1.64e-02
   Intersection ratio (m1): 9.93e-01 +- 1.06e-02
   Intersection ratio of complement (m2): 9.68e-03 +- 1.35e-02


Values printed by the script correspond to the `Jaccard index`_
(:py:func:`bob.bio.vein.preprocessor.utils.jaccard_index`), as well as the
intersection ratio between the manual and automatically generated masks
(:py:func:`bob.bio.vein.preprocessor.utils.intersect_ratio`) and the ratio to
the complement of the intersection with respect to the automatically generated
mask
(:py:func:`bob.bio.vein.preprocessor.utils.intersect_ratio_of_complement`). You
can use the option ``-n 5`` to print the 5 worst cases according to each of the
metrics.


.. _preprocessing_verafinger_nom_table:
.. table:: Performance of RoI extraction algorithms on the Verafinger database ("Nom" protocol) when compared to annotations
   :widths: auto

   ===================================== ========== ========== ==========
    Algorithm (module)                    Jaccard       M1         M2
   ===================================== ========== ========== ==========
    Annotations (benchmark_annotations)    1.0        1.0         0.0
    Watershed (benchmark_watershed)        0.98       0.99        0.01
    Tome's Lee (benchmark_tomelee)         0.96       0.98        0.02
    Lee (benchmark_lee)                    0.95       0.99        0.05
    Kono (benchmark_kono)                  0.94       0.99        0.05
   ===================================== ========== ========== ==========


.. _preprocessing_hmt_default_table:
.. table:: Performance of RoI extraction algorithms on the SDUMLA HMT vein database ("default" protocol) when compared to annotations
   :widths: auto

   ===================================== ========== ========== ==========
    Algorithm (module)                    Jaccard       M1         M2
   ===================================== ========== ========== ==========
    Annotations (benchmark_annotations)    1.0        1.0         0.0
    Watershed (benchmark_watershed)        0.98       0.99        0.02
    Tome's Lee (benchmark_tomelee)         0.88       0.92        0.04
    Lee (benchmark_lee)
    Kono (benchmark_kono)
   ===================================== ========== ========== ==========



Training the Watershed Finger region detector
---------------------------------------------

The correct detection of the finger boundaries is an important step of many
algorithms for the recognition of finger veins. It allows to compensate for
eventual rotation and scaling issues one might find when comparing models and
probes. In this package, we propose a novel finger boundary detector based on
the `Watershedding Morphological Algorithm
<https://en.wikipedia.org/wiki/Watershed_(image_processing)>`. Watershedding
works in three steps:

1. Determine markers on the original image indicating the types of areas one
   would like to detect (e.g. "finger" or "background")
2. Determine a 2D (gray-scale) surface representing the original image in which
   darker spots (representing valleys) are more likely to be filled by
   surrounding markers. This is normally achieved by filtering the image with a
   high-pass filter like Sobel or using an edge detector such as Canny.
3. Run the watershed algorithm

In order to determine markers for step 1, we train a neural network which
outputs the likelihood of a point being part of a finger, given its coordinates
and values of surrounding pixels.

When used to run an experiment,
:py:class:`bob.bio.vein.preprocessor.WatershedMask` requires you provide a
*pre-trained* neural network model that presets the markers before
watershedding takes place. In order to create one, you can run the program
`markdet.py`:

.. code-block:: sh

   $ markdet.py --hidden=20 --samples=500 fv3d central dev 3

You input, as arguments to this application, the database, protocol and subset
name you wish to use for training the network alongside the number of pixels
determining the window size around the pixel of interest the network will be
trained for (i.e., the filtering *footprint*). The data is loaded observing a
total maximum number of samples from the dataset (passed with ``--samples=N``),
the network is trained and recorded into an HDF5 file (by default, the file is
called ``model.hdf5``, but the name can be changed with the option
``--model=``).  Once you have a model, you can use the preprocessor mask by
constructing an object and attaching it to the
:py:class:`bob.bio.vein.preprocessor.Preprocessor` entry on your configuration.


.. note::

   By default, we provide pre-trained neural networks with specific
   configurations for each dataset. If not explicitly set when instantiating
   the preprocessor, these defaults are used. You can find these models under
   this package's data (see the ``data`` directory under
   ``bob.bio.vein.configurations``).


Visualizing the Watershed Finger region detector
------------------------------------------------

The utility ``watershed_mask.py`` can help you visualize the output of the
Watersheding preprocessing. By default, it displays the result of preprocessing
one sample from a database:


.. code-block:: sh

   $ watershed_mask.py verafinger 001-M/001_L_1


Should produce an image like the following:


.. figure:: img/watershed.*
   :scale: 50%

   Example RoI overlayed on finger vein image of the Vera fingervein database,
   as produced by the script ``watershed_mask.py``.


The application ``watershed_mask.py`` has more options allowing you to override
the model used for marker detection, thresholds, save resulting images from the
various processing phases or scan for thresholds. Use ``--help`` for help.


.. include:: links.rst
